\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{natbib}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{What If Without the Conformal Prediction Method}


\author{
XIONG Zhixi\\
Department of Mathematics\\
Hong Kong University of Science and Technology\\
Hong Kong, China
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
The word \textbf{Abstract} must be centered, bold, and in point size 12. Two
line spaces precede the abstract. The abstract must be limited to one
paragraph.
\end{abstract}

\section{Introduction}

In scientific, engineering, and everyday decision-making, obtaining a prediction is insufficient without also understanding its reliability through effective uncertainty quantification (UQ). Whether in autonomous driving, medical diagnosis, or financial risk assessment, relying solely on point estimates poses significant risks, as models are always imperfect and the world exhibits inherent noise. This challenge is formalized in classical decision theory, where optimal decisions require not only an expected outcome but a complete characterization of uncertainty. Formally, consider a decision problem where we observe data $x \in \mathcal{X}$ and must choose an action $a \in \mathcal{A}$. The consequence of this action depends on an unknown state $y \in \mathcal{Y}$, and is quantified by a loss function $L: \mathcal{A} \times \mathcal{Y} \to \mathbb{R}$. In the Bayesian decision theory \citep{berger1985statistical}, the optimal decision minimizes the \emph{posterior expected loss}:
\begin{equation}
    a^*(x) = \arg\min_{a \in \mathcal{A}} \mathbb{E}_{y \sim p(y|x)} \left[ L(a, y) \right] = \arg\min_{a \in \mathcal{A}} \int_{\mathcal{Y}} L(a, y) \, p(y|x) \, dy,
    \label{eq:bayesian_decision}
\end{equation}
where $p(y|x)$ is the posterior distribution of $y$ given $x$. As shown in (\ref{eq:bayesian_decision}), the optimal action $a^*$ depends not merely on a point estimate (such as the posterior mean), but on the entire posterior distribution. A poor characterization of $p(y|x)$ can lead to suboptimal decisions even with an accurate predictive model. Thus, robust and interpretable decision-making fundamentally requires well-calibrated uncertainty estimates. Consequently, a central challenge in modern machine learning, particularly for complex black-box models like deep neural networks, is to provide rigorous and reliable uncertainty measures for predictions. Without such measures, deploying these models in high-stakes domains remains risky.


To address this challenge, a variety of UQ methods have been developed, each with its own set of limitations that restrict practical applicability. Parametric models, for instance, often rely on strong distributional assumptions (e.g., normality) that are rarely justified in complex, real-world data settings \citep{wasserman2004all}. 
Asymptotic theories, while providing theoretical guarantees under ideal conditions, require sample sizes that are often unattainable in practice, and their convergence may not be guaranteed for finite or moderate datasets \citep{vovk2005algorithmic}. 
Within the Bayesian paradigm, the choice of a prior distribution can be subjective and difficult to justify. Furthermore, computing the posterior distribution is often intractable for high-dimensional or non-conjugate models, which often necessitates the use of approximate inference schemes that may themselves introduce errors \citep{murphy2012machine}.
Among more recent machine learning approaches, ensemble methods (e.g., deep ensembles \citep{lakshminarayanan2017simple}) can yield well-calibrated uncertainty estimates but at a prohibitive computational cost, as they require training and maintaining multiple models, which is a significant burden for large datasets and complex architectures. 
Techniques like Monte Carlo Dropout \citep{gal2016dropout} offer a more lightweight alternative by approximating Bayesian inference within neural networks, yet they impose specific architectural constraints (e.g., dropout layers) and require careful tuning of key hyperparameters, most notably the dropout rate, as well as the overall training schedule.
Lastly, generative models (e.g., diffusion \citep{ho2020denoising} or flow matching \citep{lipman2022flow}) can model complex data distributions but are notoriously expensive to train and may be challenging to scale to high-dimensional problems.
In summary, existing methods are often constrained by strong assumptions, high computational demands, or a lack of finite-sample guarantees, highlighting the need for a framework that is both model-agnostic and theoretically rigorous.

The conformal prediction (CP) framework emerges as a compelling solution to these limitations, offering a model-agnostic approach to uncertainty quantification with finite-sample, distribution-free guarantees \citep{LearningByTransduction, vovk2005algorithmic}. Its core mechanism relies on the concept of \emph{nonconformity scores}. For any new input, CP evaluates how nonconforming each potential output appears relative to a set of labeled reference (or calibration) data. By calibrating a threshold based on the empirical distribution of these scores, CP constructs a prediction set (for classification) or a prediction interval (for regression) that is guaranteed to contain the true outcome with a user-specified probability (e.g., 90\%), under the weak assumption of data exchangeability. This procedure essentially uses past empirical errors to quantify future uncertainty, requiring no strong distributional assumptions, no modifications to the underlying model, and no asymptotic approximations.


Originally introduced by Gammerman, Vovk, and Vapnik in 1998 for Support Vector Machines \citep{hearst1998support}, Conformal Prediction has transcended its initial scope to become a pervasive framework in modern reliable AI\@.
While theoretically grounded, its value is most evident in its rapidly expanding real-world impact across diverse high-stakes domains.
In the natural sciences, CP is now pivotal for guiding protein design sequences \citep{fannjiang2022conformal}.
In the realm of robotics and embodied AI, it facilitates safe planning in dynamic environments \citep{lindemann2023safe} and has been successfully applied to align uncertainty in Large Language Models (LLMs) for robotic interaction \citep{ren2023robots}.
The framework's reliability is equally critical in healthcare, where it has been deployed for estimating diagnostic uncertainty in cancer pathology \citep{olsson2022estimating} and predicting disease courses for conditions such as multiple sclerosis \citep{sreenivasan2025conformal}.
Additionally, CP has found utility in earth observation \citep{singh2024uncertainty}, further demonstrating its versatility and robustness in securing trust for safety-critical applications.
Collectively, these advancements illustrate the transformative potential of CP in bridging the gap between complex, black-box algorithms and the rigorous safety standards demanded by real-world applications.



\section{Methods}
\label{sec:methods}

This section formalizes the CP framework. We begin with the general transductive formulation, proceed to the computationally efficient Split Conformal Prediction, and conclude by discussing advanced variants designed to address limitations regarding adaptivity, distribution shifts, and conditional coverage.

\subsection{General Conformal Prediction Framework}

Consider a regression or classification problem where we observe a sequence of data points $Z_1, \dots, Z_n$, where $Z_i = (X_i, Y_i) \in \mathcal{X} \times \mathcal{Y}$. We assume these data points are \emph{exchangeable}, meaning their joint distribution is invariant under any permutation. Given a new test input $X_{n+1}$, our goal is to construct a prediction set $C(X_{n+1}) \subseteq \mathcal{Y}$ that covers the unknown true label $Y_{n+1}$ with a user-specified probability $1-\alpha$, where $\alpha \in (0, 1)$ is the miscoverage rate.

The general conformal prediction framework, often referred to as Full or Transductive CP \citep{vovk2005algorithmic}, relies on a \emph{nonconformity measure} $S: \mathcal{Z} \to \mathbb{R}$. This function assigns a score $s_i = S(Z_i)$ representing how ``unusual'' a data point $Z_i$ is relative to the others. 
For a candidate label $y \in \mathcal{Y}$ paired with $X_{n+1}$, we form a hypothetical dataset including $Z_{n+1} = (X_{n+1}, y)$. We then compute nonconformity scores for all $i \in \{1, \dots, n+1\}$. A $p$-value for the candidate $y$ is derived by comparing its score to the scores of the existing data:
\begin{equation}
    \pi(y) = \frac{1}{n+1} \sum_{i=1}^{n+1} \mathbb{I}\left( s_i \geq s_{n+1} \right),
    \label{eq:p_value}
\end{equation}
where $\mathbb{I}(\cdot)$ is the indicator function. The prediction set is constructed by including all candidates $y$ that appear statistically plausible:
\begin{equation}
    C_{\text{Full}}(X_{n+1}) = \left\{ y \in \mathcal{Y} \mid \pi(y) > \alpha \right\}.
    \label{eq:full_cp_set}
\end{equation}
Under the exchangeability assumption, this set satisfies the marginal validity property:
\begin{equation}
\mathbb{P}(Y_{n+1} \in C_{\text{Full}}(X_{n+1})) \ge 1-\alpha,
\end{equation}
for any sample size $n$ and any underlying distribution. The structure of the set $C_{\text{Full}}(X_{n+1})$ depends on the nature of the output space $\mathcal{Y}$:

\paragraph{Classification.} When $\mathcal{Y}$ is a finite set of discrete labels (e.g., $\{1, \dots, K\}$), we can explicitly calculate $\pi(y)$ for each possible class. The prediction set is simply the subset of labels for which the $p$-value exceeds $\alpha$.

\paragraph{Regression.} When $\mathcal{Y} = \mathbb{R}$, iterating through all possible values of $y$ is computationally impossible. However, for standard nonconformity measures (such as the absolute error $|y - \hat{\mu}(X_{n+1})|$), the function $\pi(y)$ is generally quasi-concave with respect to $y$. Consequently, the set defined in (\ref{eq:full_cp_set}) typically forms a continuous interval (or a union of intervals). In practice, this interval is computed by inverting the nonconformity score function to find the boundaries of $y$ that satisfy the condition $\pi(y) > \alpha$.

\subsection{Split Conformal Prediction}

While theoretically robust, Full CP is computationally prohibitive for complex models (e.g., neural networks) because it requires retraining the underlying model for every candidate $y$ to compute the scores $s_i$ properly. To address this computational bottleneck, \emph{Split Conformal Prediction} (SCP), also known as Inductive CP, is the most widely adopted variant in modern machine learning \citep{papadopoulos2002inductive, lei2018distribution}.

In SCP, the available data is partitioned into two disjoint subsets: a proper training set $\mathcal{D}_{\text{train}}$ and a calibration set $\mathcal{D}_{\text{cal}} = \{(X_i, Y_i)\}_{i=1}^{n}$. A predictive model is trained solely on $\mathcal{D}_{\text{train}}$. We then compute nonconformity scores on $\mathcal{D}_{\text{cal}}$ using this fixed model. The construction of the prediction set depends on the task type:

\paragraph{Classification.}
For a classification task with discrete labels $\mathcal{Y} = \{1, \dots, K\}$, the model $\hat{f}: \mathcal{X} \to [0, 1]^K$ typically outputs a probability distribution over classes, where $\hat{f}(x)_k$ denotes the estimated probability of class $k$. A common nonconformity score is the complement of the probability assigned to the true class:
\begin{equation}
    s_i = 1 - \hat{f}(X_i)_{Y_i}, \quad \forall i \in \mathcal{D}_{\text{cal}}.
\end{equation}
Let $\hat{q}$ be the $\lceil (n+1)(1-\alpha) \rceil / n$ empirical quantile of the calibration scores $\{s_1, \dots, s_n\}$. The prediction set includes all classes with a predicted probability above the calibrated threshold:
\begin{equation}
    C_{\text{SCP}}(X_{n+1}) = \left\{ k \in \mathcal{Y} \mid \hat{f}(X_{n+1})_k \ge 1 - \hat{q} \right\}.
    \label{eq:scp_set_class}
\end{equation}
This formulation, often referred to as Least Ambiguous Set-valued Classifiers \citep{sadinle2019least}, guarantees that the true label is included in the set with probability at least $1-\alpha$.

\paragraph{Regression.}
For a regression task ($\mathcal{Y} = \mathbb{R}$), let $\hat{\mu}: \mathcal{X} \to \mathbb{R}$ be the trained model. A standard choice for the nonconformity score is the absolute residual:
\begin{equation}
    s_i = |Y_i - \hat{\mu}(X_i)|, \quad \forall i \in \mathcal{D}_{\text{cal}}.
\end{equation}
Similarly, let $\hat{q}$ be the $\lceil (n+1)(1-\alpha) \rceil / n$ empirical quantile of these regression scores. The prediction set for a new input $X_{n+1}$ is constructed as a fixed-width interval:
\begin{equation}
    C_{\text{SCP}}(X_{n+1}) = \left[ \hat{\mu}(X_{n+1}) - \hat{q}, \quad \hat{\mu}(X_{n+1}) + \hat{q} \right].
    \label{eq:scp_interval}
\end{equation}

\vspace{0.5em}
In both cases, SCP maintains the finite-sample validity guarantee provided the calibration and test data are exchangeable. Because the model is trained only once, SCP is computationally efficient and model-agnostic.

\subsection{Advanced Variants addressing Limitations}

While SCP provides rigorous marginal coverage, the formulation in (\ref{eq:scp_interval}) constructs intervals of fixed width $2\hat{q}$ for all inputs. This assumes homoscedasticity, which is rarely true in real-world data where uncertainty often varies with the input $X$ (heteroscedasticity). To address this and other limitations, several key variants have been developed.

\paragraph{Locally Adaptive Conformal Prediction.} Standard SCP often results in inefficient sets (too wide for easy inputs, too narrow for hard ones). To mitigate this, \emph{Conformalized Quantile Regression} (CQR) \citep{romano2019conformal} was proposed to construct intervals that adapt their width to the local difficulty of the input. The CQR procedure consists of two main steps: quantile regression training and conformal calibration.

First, using the training set $\mathcal{D}_{\text{train}}$, we train a regression model $\hat{f}$ to estimate two conditional quantiles: the lower quantile at level $\gamma_{\text{lo}} = \alpha/2$ and the upper quantile at level $\gamma_{\text{hi}} = 1 - \alpha/2$. The model outputs $\hat{q}_{\text{lo}}(x)$ and $\hat{q}_{\text{hi}}(x)$, which are optimized by minimizing the pinball loss (or quantile loss) \citep{koenker2001quantile}:
\begin{equation}
    \mathcal{L}(y, \hat{y}, \gamma) = \max \left( \gamma (y - \hat{y}), (\gamma - 1) (y - \hat{y}) \right).
\end{equation}
The total objective minimizes the average loss over both quantiles: $\sum_{i \in \mathcal{D}_{\text{train}}} [\mathcal{L}(y_i, \hat{q}_{\text{lo}}(x_i), \gamma_{\text{lo}}) + \mathcal{L}(y_i, \hat{q}_{\text{hi}}(x_i), \gamma_{\text{hi}})]$.

Second, although these raw quantile estimates provide a heuristic interval $[\hat{q}_{\text{lo}}(X), \hat{q}_{\text{hi}}(X)]$, they do not guarantee finite-sample coverage. CQR acts as a ``wrapper'' to rigorously calibrate them. We compute nonconformity scores on the calibration set $\mathcal{D}_{\text{cal}}$ as:
\begin{equation}
    s_i = \max \left( \hat{q}_{\text{lo}}(X_i) - Y_i, \; Y_i - \hat{q}_{\text{hi}}(X_i) \right).
\end{equation}
Intuitively, $s_i$ measures the signed distance from the true label $Y_i$ to the nearest boundary of the predicted interval. If $Y_i$ falls inside the interval, $s_i$ is negative; if it falls outside, $s_i$ is positive.

Let $\hat{q}$ be the $\lceil (n+1)(1-\alpha) \rceil / n$ empirical quantile of these scores $\{s_1, \dots, s_n\}$. The final conformalized prediction interval for a new input $X_{n+1}$ is constructed by expanding (or shrinking) the raw quantile estimates by $\hat{q}$:
\begin{equation}
    C_{\text{CQR}}(X_{n+1}) = \left[ \hat{q}_{\text{lo}}(X_{n+1}) - \hat{q}, \quad \hat{q}_{\text{hi}}(X_{n+1}) + \hat{q} \right].
\end{equation}
This procedure ensures valid coverage while allowing the interval width $\hat{q}_{\text{hi}}(X) - \hat{q}_{\text{lo}}(X) + 2\hat{q}$ to vary dynamically based on the input $X$, significantly improving informativeness in heteroscedastic settings.

\paragraph{Covariate Shift and Weighted Conformal Prediction.}
The standard exchangeability assumption is violated under distribution shift, where the test distribution $P_{\text{test}}(X)$ differs from the training distribution $P_{\text{train}}(X)$. Under such \emph{covariate shift}, standard CP loses its coverage guarantee. \cite{tibshirani2019conformal} proposed \emph{Weighted Conformal Prediction} (WCP), which adjusts the quantile calculation using likelihood ratios. For each calibration point $X_i \in \mathcal{D}_{\text{cal}}$, we assign a weight proportional to the density ratio:
\begin{equation}
    w(X_i) = \frac{dP_{\text{test}}(X_i)}{dP_{\text{train}}(X_i)} = \frac{p_{\text{test}}(X_i)}{p_{\text{train}}(X_i)},
\end{equation}
where $p_{\text{test}}$ and $p_{\text{train}}$ denote the probability density functions (or probability mass functions) of the test and training distributions, respectively. Intuitively, $w(X_i)$ quantifies how much more likely the input $X_i$ is to appear in the test environment compared to the training environment.

Given these weights, WCP modifies the standard calibration procedure by replacing the uniform empirical distribution with a weighted counterpart. Specifically, after computing nonconformity scores $\{s_i\}_{i=1}^n$ on the calibration set, we assign a normalized probability mass to each point. For a new test input $X_{n+1}$, the calibration points are weighted by $w(X_i)$, while the test point itself is assigned a fixed weight of 1 (reflecting its draw from the target distribution). The resulting probability weights are defined as:
\begin{equation}
    p_i(X_{n+1}) = \frac{w(X_i)}{\sum_{j=1}^n w(X_j) + 1}, \quad p_{n+1}(X_{n+1}) = \frac{1}{\sum_{j=1}^n w(X_j) + 1}.
\end{equation}
These probabilities are then used to construct the weighted empirical cumulative distribution function of the scores:
\begin{equation}
    \hat{F}(s) = \sum_{i=1}^n p_i(X_{n+1}) \, \mathbb{I}(s_i \le s) + p_{n+1}(X_{n+1}) \, \mathbb{I}(\infty \le s).
\end{equation}
The calibrated threshold $\hat{q}$ is determined as the smallest score $s$ such that $\hat{F}(s) \ge 1-\alpha$. Consequently, the final prediction set is formed as $C_{\text{WCP}}(X_{n+1}) = [\hat{\mu}(X_{n+1}) - \hat{q}, \; \hat{\mu}(X_{n+1}) + \hat{q}]$. By incorporating the likelihood ratio into the quantile estimation, this method ensures that the coverage guarantee holds with respect to the target distribution $P_{\text{test}}$.

\paragraph{Conditional Coverage and Mondrian CP.}
SCP guarantees \emph{marginal} coverage (on average over all inputs), but may systematically under-cover for specific subgroups of the data. To address the need for \emph{conditional} coverage (i.e., validity within specific classes or groups), \emph{Mondrian Conformal Prediction} \citep{vovk2005algorithmic} partitions the calibration data into categories (or "bins") and performs calibration separately within each bin. This ensures that the coverage guarantee holds independently for each defined subgroup, preventing the model from sacrificing accuracy on minority classes to satisfy the global average.






\section{Headings: first level}
\label{headings}

First level headings are lower case (except for first word and proper nouns),
flush left, bold and in point size 12. One line space before the first level
heading and 1/2~line space after the first level heading.

\subsection{Headings: second level}

Second level headings are lower case (except for first word and proper nouns),
flush left, bold and in point size 10. One line space before the second level
heading and 1/2~line space after the second level heading.

\subsubsection{Headings: third level}

Third level headings are lower case (except for first word and proper nouns),
flush left, bold and in point size 10. One line space before the third level
heading and 1/2~line space after the third level heading.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\subsection{Citations within the text}

Citations within the text should be numbered consecutively. The corresponding
number is to appear enclosed in square brackets, such as [1] or [2]-[5]. The
corresponding references are to be listed in the same order at the end of the
paper, in the \textbf{References} section. (Note: the standard
\textsc{Bib\TeX} style \texttt{unsrt} produces this.) As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

As submission is double blind, refer to your own published work in the 
third person. That is, use ``In the previous work of Jones et al.\ [4]'',
not ``In our previous work [4]''. If you cite your other papers that
are not widely available (e.g.\ a journal paper under review), use
anonymous author names in the citation, e.g.\ an author of the
form ``A.\ Anonymous''. 


\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures. 
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.
\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{Final instructions}
Do not change any aspects of the formatting parameters in the style files.
In particular, do not modify the width or length of the rectangle the text
should fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.

\section{Preparing PostScript or PDF files}

Please prepare PostScript or PDF files with paper size ``US Letter'', and
not, for example, ``A4''. The -t
letter option on dvips will produce US Letter files.

Fonts were the main cause of problems in the past years. Your PDF file must
only contain Type 1 or Embedded TrueType fonts. Here are a few instructions
to achieve this.

\begin{itemize}

\item You can check which fonts a PDF files uses.  In Acrobat Reader,
select the menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
available out-of-the-box on most Linux machines.

\item The IEEE has recommendations for generating PDF files whose fonts
are also acceptable for NIPS. Please see
\url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

\item LaTeX users:

\begin{itemize}

\item Consider directly generating PDF files using \verb+pdflatex+
(especially if you are a MiKTeX user). 
PDF figures must be substituted for EPS figures, however.

\item Otherwise, please generate your PostScript and PDF files with the following commands:
\begin{verbatim} 
dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
ps2pdf mypaper.ps mypaper.pdf
\end{verbatim}

Check that the PDF files only contains Type 1 fonts. 
%For the final version, please send us both the Postscript file and
%the PDF file. 

\item xfig "patterned" shapes are implemented with 
bitmap fonts.  Use "solid" shapes instead. 
\item The \verb+\bbold+ package almost always uses bitmap
fonts.  You can try the equivalent AMS Fonts with command
\begin{verbatim}
\usepackage[psamsfonts]{amssymb}
\end{verbatim}
 or use the following workaround for reals, natural and complex: 
\begin{verbatim}
\newcommand{\RR}{I\!\!R} %real numbers
\newcommand{\Nat}{I\!\!N} %natural numbers 
\newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}

\item Sometimes the problematic fonts are used in figures
included in LaTeX files. The ghostscript program \verb+eps2eps+ is the simplest
way to clean such figures. For black and white figures, slightly better
results can be achieved with program \verb+potrace+.
\end{itemize}
\item MSWord and Windows users (via PDF file):
\begin{itemize}
\item Install the Microsoft Save as PDF Office 2007 Add-in from
\url{http://www.microsoft.com/downloads/details.aspx?displaylang=en\&familyid=4d951911-3e7e-4ae6-b059-a2e79ed87041}
\item Select ``Save or Publish to PDF'' from the Office or File menu
\end{itemize}
\item MSWord and Mac OS X users (via PDF file):
\begin{itemize}
\item From the print menu, click the PDF drop-down box, and select ``Save
as PDF...''
\end{itemize}
\item MSWord and Windows users (via PS file):
\begin{itemize}
\item To create a new printer
on your computer, install the AdobePS printer driver and the Adobe Distiller PPD file from
\url{http://www.adobe.com/support/downloads/detail.jsp?ftpID=204} {\it Note:} You must reboot your PC after installing the
AdobePS driver for it to take effect.
\item To produce the ps file, select ``Print'' from the MS app, choose
the installed AdobePS printer, click on ``Properties'', click on ``Advanced.''
\item Set ``TrueType Font'' to be ``Download as Softfont''
\item Open the ``PostScript Options'' folder
\item Select ``PostScript Output Option'' to be ``Optimize for Portability''
\item Select ``TrueType Font Download Option'' to be ``Outline''
\item Select ``Send PostScript Error Handler'' to be ``No''
\item Click ``OK'' three times, print your file.
\item Now, use Adobe Acrobat Distiller or ps2pdf to create a PDF file from
the PS file. In Acrobat, check the option ``Embed all fonts'' if
applicable.
\end{itemize}

\end{itemize}
If your file contains Type 3 fonts or non embedded TrueType fonts, we will
ask you to fix it. 

\subsection{Margins in LaTeX}
 
Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+
from the graphicx package. Always specify the figure width as a multiple of
the line width as in the example below using .eps graphics
\begin{verbatim}
   \usepackage[dvips]{graphicx} ... 
   \includegraphics[width=0.8\linewidth]{myfile.eps} 
\end{verbatim}
or % Apr 2009 addition
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ... 
   \includegraphics[width=0.8\linewidth]{myfile.pdf} 
\end{verbatim}
for .pdf graphics. 
See section 4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps}) 
 
A number of width problems arise when LaTeX cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command.


\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include 
acknowledgments in the anonymized submission, only in the 
final paper. 

\subsubsection*{References}

References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point) 
when listing the references. {\bf Remember that this year you can use
a ninth page as long as it contains \emph{only} cited references.}

\bibliographystyle{plainnat} % 或 unsrtnat, abbrvnat 等
\bibliography{references}

\end{document}