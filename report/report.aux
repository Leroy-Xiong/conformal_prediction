\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{berger1985statistical}
\citation{wasserman2004all}
\citation{vovk2005algorithmic}
\citation{murphy2012machine}
\citation{lakshminarayanan2017simple}
\citation{gal2016dropout}
\citation{ho2020denoising}
\citation{lipman2022flow}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{eq:bayesian_decision}{{1}{1}{Introduction}{equation.1}{}}
\citation{LearningByTransduction,vovk2005algorithmic}
\citation{hearst1998support}
\citation{fannjiang2022conformal}
\citation{lindemann2023safe}
\citation{ren2023robots}
\citation{olsson2022estimating}
\citation{sreenivasan2025conformal}
\citation{singh2024uncertainty}
\citation{vovk2005algorithmic}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}\protected@file@percent }
\newlabel{sec:methods}{{2}{2}{Methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}General Conformal Prediction Framework}{2}{subsection.2.1}\protected@file@percent }
\newlabel{eq:p_value}{{2}{2}{General Conformal Prediction Framework}{equation.2}{}}
\citation{papadopoulos2002inductive,lei2018distribution}
\citation{sadinle2019least}
\newlabel{eq:full_cp_set}{{3}{3}{General Conformal Prediction Framework}{equation.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Classification.}{3}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regression.}{3}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Split Conformal Prediction}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Classification.}{3}{section*.3}\protected@file@percent }
\newlabel{eq:scp_set_class}{{6}{3}{Classification}{equation.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Regression.}{3}{section*.4}\protected@file@percent }
\newlabel{eq:scp_interval}{{8}{3}{Regression}{equation.8}{}}
\citation{romano2019conformal}
\citation{koenker2001quantile}
\citation{tibshirani2019conformal}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Advanced Variants addressing Limitations}{4}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Heteroscedasticity and Locally Adaptive Conformal Prediction.}{4}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Covariate Shift and Weighted Conformal Prediction.}{4}{section*.6}\protected@file@percent }
\citation{vovk2005algorithmic}
\@writefile{toc}{\contentsline {paragraph}{Conditional Coverage and Mondrian Conformal Prediction.}{5}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Advantages and Impact}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Finite-Sample Coverage}{5}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Validation of Finite-Sample Guarantee on the Diabetes Dataset.} \textbf  {(a)} The distribution of the dataset (BMI vs. Target). \textbf  {(b)} A snapshot of a single trial comparing Raw QR bounds (red dashed lines) with CP intervals (green band). The yellow stars indicate test points that fell outside the raw model's bounds but were successfully captured by the CP correction ($\hat  {q}$ adjustment). \textbf  {(c)} Distribution of empirical coverage rates over 100 trials. The Raw QR method (red) consistently under-covers (mean $\approx $ 87.8\%), failing to meet the 90\% target (dashed line) due to finite-sample errors. In contrast, CQR (green) rigorously satisfies the validity property (mean $\approx $ 92.3\%). \textbf  {(d)} The boxplot of interval widths shows that CP achieves validity by appropriately expanding the intervals to account for the uncertainty inherent in the small calibration set.}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:finite_sample_experiment}{{1}{6}{\textbf {Validation of Finite-Sample Guarantee on the Diabetes Dataset.} \textbf {(a)} The distribution of the dataset (BMI vs. Target). \textbf {(b)} A snapshot of a single trial comparing Raw QR bounds (red dashed lines) with CP intervals (green band). The yellow stars indicate test points that fell outside the raw model's bounds but were successfully captured by the CP correction ($\hat {q}$ adjustment). \textbf {(c)} Distribution of empirical coverage rates over 100 trials. The Raw QR method (red) consistently under-covers (mean $\approx $ 87.8\%), failing to meet the 90\% target (dashed line) due to finite-sample errors. In contrast, CQR (green) rigorously satisfies the validity property (mean $\approx $ 92.3\%). \textbf {(d)} The boxplot of interval widths shows that CP achieves validity by appropriately expanding the intervals to account for the uncertainty inherent in the small calibration set}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Distribution-Free}{7}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Validation of Distribution-Free Robustness.} \textbf  {(a)} Histogram of the actual residuals (blue) versus the Gaussian fit (red dashed line) assumed by the baseline method. The clear skewness and heavy tail of the actual data violate the normality assumption, leading to a poor fit. \textbf  {(b)} Comparison of empirical coverage rates. The Gaussian baseline (yellow) fails to capture the true distribution's shape, resulting in unpredictable coverage. In contrast, CP (green) achieves near-perfect validity (90.8\%) by relying on the empirical quantiles of the residuals rather than a density estimate. \textbf  {(c)} Visualization of CP intervals for the first 50 test samples. The intervals (green bars) effectively contain the true targets (black crosses) even in the presence of asymmetric noise, with failures (red crosses) occurring at the expected rate of $\alpha =10\%$.}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:dist_free_experiment}{{2}{7}{\textbf {Validation of Distribution-Free Robustness.} \textbf {(a)} Histogram of the actual residuals (blue) versus the Gaussian fit (red dashed line) assumed by the baseline method. The clear skewness and heavy tail of the actual data violate the normality assumption, leading to a poor fit. \textbf {(b)} Comparison of empirical coverage rates. The Gaussian baseline (yellow) fails to capture the true distribution's shape, resulting in unpredictable coverage. In contrast, CP (green) achieves near-perfect validity (90.8\%) by relying on the empirical quantiles of the residuals rather than a density estimate. \textbf {(c)} Visualization of CP intervals for the first 50 test samples. The intervals (green bars) effectively contain the true targets (black crosses) even in the presence of asymmetric noise, with failures (red crosses) occurring at the expected rate of $\alpha =10\%$}{figure.2}{}}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{berger1985statistical}{{1}{1985}{{Berger}}{{}}}
\bibcite{fannjiang2022conformal}{{2}{2022}{{Fannjiang et~al.}}{{Fannjiang, Bates, Angelopoulos, Listgarten, and Jordan}}}
\bibcite{gal2016dropout}{{3}{2016}{{Gal and Ghahramani}}{{}}}
\bibcite{LearningByTransduction}{{4}{1998}{{Gammerman et~al.}}{{Gammerman, Vovk, and Vapnik}}}
\bibcite{hearst1998support}{{5}{1998}{{Hearst et~al.}}{{Hearst, Dumais, Osuna, Platt, and Scholkopf}}}
\bibcite{ho2020denoising}{{6}{2020}{{Ho et~al.}}{{Ho, Jain, and Abbeel}}}
\bibcite{koenker2001quantile}{{7}{2001}{{Koenker and Hallock}}{{}}}
\bibcite{lakshminarayanan2017simple}{{8}{2017}{{Lakshminarayanan et~al.}}{{Lakshminarayanan, Pritzel, and Blundell}}}
\bibcite{lei2018distribution}{{9}{2018}{{Lei et~al.}}{{Lei, G'Sell, Rinaldo, Tibshirani, and Wasserman}}}
\bibcite{lindemann2023safe}{{10}{2023}{{Lindemann et~al.}}{{Lindemann, Cleaveland, Shim, and Pappas}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Model-Free}{8}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Comparative Analysis}{8}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}\protected@file@percent }
\bibcite{lipman2022flow}{{11}{2022}{{Lipman et~al.}}{{Lipman, Chen, Ben-Hamu, Nickel, and Le}}}
\bibcite{murphy2012machine}{{12}{2012}{{Murphy}}{{}}}
\bibcite{olsson2022estimating}{{13}{2022}{{Olsson et~al.}}{{Olsson, Kartasalo, Mulliqi, Capuccini, Ruusuvuori, Samaratunga, Delahunt, Lindskog, Janssen, Blilie, et~al.}}}
\bibcite{papadopoulos2002inductive}{{14}{2002}{{Papadopoulos et~al.}}{{Papadopoulos, Proedrou, Vovk, and Gammerman}}}
\bibcite{ren2023robots}{{15}{2023}{{Ren et~al.}}{{Ren, Dixit, Bodrova, Singh, Tu, Brown, Xu, Takayama, Xia, Varley, et~al.}}}
\bibcite{romano2019conformal}{{16}{2019}{{Romano et~al.}}{{Romano, Patterson, and Candes}}}
\bibcite{sadinle2019least}{{17}{2019}{{Sadinle et~al.}}{{Sadinle, Lei, and Wasserman}}}
\bibcite{singh2024uncertainty}{{18}{2024}{{Singh et~al.}}{{Singh, Moncrieff, Venter, Cawse-Nicholson, Slingsby, and Robinson}}}
\bibcite{sreenivasan2025conformal}{{19}{2025}{{Sreenivasan et~al.}}{{Sreenivasan, Vaivade, Noui, Khoonsari, Burman, Spjuth, and Kultima}}}
\bibcite{tibshirani2019conformal}{{20}{2019}{{Tibshirani et~al.}}{{Tibshirani, Barber, Candes, and Ramdas}}}
\bibcite{vovk2005algorithmic}{{21}{2005}{{Vovk et~al.}}{{Vovk, Gammerman, and Shafer}}}
\bibcite{wasserman2004all}{{22}{2004}{{Wasserman}}{{}}}
\gdef \@abspage@last{9}
