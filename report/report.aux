\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{berger1985statistical}
\citation{wasserman2004all}
\citation{vovk2005algorithmic}
\citation{murphy2012machine}
\citation{lakshminarayanan2017simple}
\citation{gal2016dropout}
\citation{ho2020denoising}
\citation{lipman2022flow}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{eq:bayesian_decision}{{1}{1}{Introduction}{equation.1}{}}
\citation{LearningByTransduction,vovk2005algorithmic}
\citation{hearst1998support}
\citation{fannjiang2022conformal}
\citation{lindemann2023safe}
\citation{ren2023robots}
\citation{olsson2022estimating}
\citation{sreenivasan2025conformal}
\citation{singh2024uncertainty}
\citation{vovk2005algorithmic}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}\protected@file@percent }
\newlabel{sec:methods}{{2}{2}{Methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}General Conformal Prediction Framework}{2}{subsection.2.1}\protected@file@percent }
\newlabel{eq:p_value}{{2}{2}{General Conformal Prediction Framework}{equation.2}{}}
\newlabel{eq:full_cp_set}{{3}{2}{General Conformal Prediction Framework}{equation.3}{}}
\citation{papadopoulos2002inductive,lei2018distribution}
\citation{sadinle2019least}
\@writefile{toc}{\contentsline {paragraph}{Classification.}{3}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regression.}{3}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Split Conformal Prediction}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Classification.}{3}{section*.3}\protected@file@percent }
\newlabel{eq:scp_set_class}{{6}{3}{Classification}{equation.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Regression.}{3}{section*.4}\protected@file@percent }
\newlabel{eq:scp_interval}{{8}{3}{Regression}{equation.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Advanced Variants addressing Limitations}{3}{subsection.2.3}\protected@file@percent }
\citation{romano2019conformal}
\citation{koenker2001quantile}
\citation{tibshirani2019conformal}
\@writefile{toc}{\contentsline {paragraph}{Heteroscedasticity and Locally Adaptive Conformal Prediction.}{4}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Covariate Shift and Weighted Conformal Prediction.}{4}{section*.6}\protected@file@percent }
\citation{vovk2005algorithmic}
\@writefile{toc}{\contentsline {paragraph}{Conditional Coverage and Mondrian Conformal Prediction.}{5}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Advantages and Impact}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Finite-Sample Coverage}{5}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Validation of Finite-Sample Guarantee on the Diabetes Dataset.} \textbf  {(a)} The distribution of the dataset (BMI vs. Target). \textbf  {(b)} A snapshot of a single trial comparing Raw QR bounds (red dashed lines) with CP intervals (green band). The yellow stars indicate test points that fell outside the raw model's bounds but were successfully captured by the CP correction ($\hat  {q}$ adjustment). \textbf  {(c)} Distribution of empirical coverage rates over 100 trials. The Raw QR method (red) consistently under-covers (mean $\approx $ 87.8\%), failing to meet the 90\% target (dashed line) due to finite-sample errors. In contrast, CQR (green) rigorously satisfies the validity property (mean $\approx $ 92.3\%). \textbf  {(d)} The boxplot of interval widths shows that CP achieves validity by appropriately expanding the intervals to account for the uncertainty inherent in the small calibration set.}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:finite_sample_experiment}{{1}{6}{\textbf {Validation of Finite-Sample Guarantee on the Diabetes Dataset.} \textbf {(a)} The distribution of the dataset (BMI vs. Target). \textbf {(b)} A snapshot of a single trial comparing Raw QR bounds (red dashed lines) with CP intervals (green band). The yellow stars indicate test points that fell outside the raw model's bounds but were successfully captured by the CP correction ($\hat {q}$ adjustment). \textbf {(c)} Distribution of empirical coverage rates over 100 trials. The Raw QR method (red) consistently under-covers (mean $\approx $ 87.8\%), failing to meet the 90\% target (dashed line) due to finite-sample errors. In contrast, CQR (green) rigorously satisfies the validity property (mean $\approx $ 92.3\%). \textbf {(d)} The boxplot of interval widths shows that CP achieves validity by appropriately expanding the intervals to account for the uncertainty inherent in the small calibration set}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Distribution-Free}{7}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Model-Free}{7}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Validation of Distribution-Free Robustness.} \textbf  {(a)} Histogram of the actual residuals (blue) versus the Gaussian fit (red dashed line) assumed by the baseline method. The clear skewness and heavy tail of the actual data violate the normality assumption, leading to a poor fit. \textbf  {(b)} Comparison of empirical coverage rates. The Gaussian baseline (yellow) fails to capture the true distribution's shape, resulting in unpredictable coverage. In contrast, CP (green) achieves near-perfect validity (90.8\%) by relying on the empirical quantiles of the residuals rather than a density estimate. \textbf  {(c)} Visualization of CP intervals for the first 50 test samples. The intervals (green bars) effectively contain the true targets (black crosses) even in the presence of asymmetric noise, with failures (red crosses) occurring at the expected rate of $\alpha =10\%$.}}{8}{figure.2}\protected@file@percent }
\newlabel{fig:dist_free_experiment}{{2}{8}{\textbf {Validation of Distribution-Free Robustness.} \textbf {(a)} Histogram of the actual residuals (blue) versus the Gaussian fit (red dashed line) assumed by the baseline method. The clear skewness and heavy tail of the actual data violate the normality assumption, leading to a poor fit. \textbf {(b)} Comparison of empirical coverage rates. The Gaussian baseline (yellow) fails to capture the true distribution's shape, resulting in unpredictable coverage. In contrast, CP (green) achieves near-perfect validity (90.8\%) by relying on the empirical quantiles of the residuals rather than a density estimate. \textbf {(c)} Visualization of CP intervals for the first 50 test samples. The intervals (green bars) effectively contain the true targets (black crosses) even in the presence of asymmetric noise, with failures (red crosses) occurring at the expected rate of $\alpha =10\%$}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Evaluation of Model Agnosticism across 7 Architectures.} \textbf  {(a)} Models sorted by predictive error (MSE), ranging from weaker models (KNN, Linear) to strong ensembles (Gradient Boosting). \textbf  {(b)} The average prediction interval width follows the trend of the MSE\spacefactor \@m {}. Stronger models like Gradient Boosting produce significantly sharper intervals (17.7 units) compared to the Linear baseline (37.5 units). \textbf  {(c)} Empirical coverage rates for all models. Despite the vast differences in underlying mathematical principles and accuracy, CP ensures that every model approximates the 90\% target (dashed line), validating the model-free property.}}{9}{figure.3}\protected@file@percent }
\newlabel{fig:model_free_results}{{3}{9}{\textbf {Evaluation of Model Agnosticism across 7 Architectures.} \textbf {(a)} Models sorted by predictive error (MSE), ranging from weaker models (KNN, Linear) to strong ensembles (Gradient Boosting). \textbf {(b)} The average prediction interval width follows the trend of the MSE\@. Stronger models like Gradient Boosting produce significantly sharper intervals (17.7 units) compared to the Linear baseline (37.5 units). \textbf {(c)} Empirical coverage rates for all models. Despite the vast differences in underlying mathematical principles and accuracy, CP ensures that every model approximates the 90\% target (dashed line), validating the model-free property}{figure.3}{}}
\bibstyle{plainnat}
\bibdata{references}
\@writefile{toc}{\contentsline {section}{\numberline {4}Comparative Analysis}{10}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{10}{section.5}\protected@file@percent }
\gdef \@abspage@last{10}
